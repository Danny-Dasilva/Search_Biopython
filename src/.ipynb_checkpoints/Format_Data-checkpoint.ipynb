{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_and_clean_documents import *\n",
    "from text_processing import *\n",
    "from clustering_functions import *\n",
    "from plot import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_FOLDER = \"../data/\"\n",
    "HTML_DATA_FOLDER = \"test.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_cleaned_html_documents(folder_file_list):\n",
    "    \"\"\"\n",
    "    Reads the files in folders\n",
    "    Returns a list of list containing cleaned words/data in the html files\n",
    "\n",
    "    Param_1: Output of get_file_names, list of folders and file names\n",
    "    Output_1: List of lists containing string, which are words in the html\n",
    "    \"\"\"\n",
    "    html_parser = HTMLParser()\n",
    "    cleaned_documents = []\n",
    "    for item in folder_file_list:\n",
    "        for file_name in item[1]:\n",
    "            f = open(file_name, \"r\")  # Open the file\n",
    "            x = f.read()  # Read file\n",
    "            html_parser.feed(x)  # Feed the file to get rid of html elements\n",
    "            f.close()  # Close the file\n",
    "        plain_html_file = html_parser.html_plain_document  # Get the list of words\n",
    "        cleaned_html_file = clean_list(plain_html_file)  # Clean the list\n",
    "        cleaned_documents.append(cleaned_html_file)\n",
    "        html_parser.html_plain_document = []  # Empty the html document\n",
    "    return cleaned_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "class MyHTMLParser(HTMLParser):  # To parse html files\n",
    "    html_plain_document = []\n",
    "    def handle_data(self, data):  # For every html data\n",
    "        self.html_plain_document.append(data)  # Append the data to the list\n",
    "\n",
    "def clean_list(list_to_clean):\n",
    "    \"\"\"\n",
    "    Function to clean a list\n",
    "    Removes any non-alphanumeric characters\n",
    "    Stems words\n",
    "    Gets rid of any empty elements in the list\n",
    "\n",
    "    Param_1: List, containing strings\n",
    "    Output_1: List, containing cleaned strings\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    items_to_clean = set(list(stopwords.words('english')) + ['\\n','\\n\\n','\\n\\n\\n','\\n\\n\\n\\n','ocroutput','',' '])\n",
    "    # Items to clean\n",
    "    print(items_to_clean)\n",
    "    regex_non_alphanumeric = re.compile('[^0-9a-zA-Z]')  # REGEX for non alphanumeric chars\n",
    "    for index,item in enumerate(list_to_clean):\n",
    "        item = regex_non_alphanumeric.sub('', item)  # Filter text, remove non alphanumeric chars\n",
    "        item = item.lower()  # Lowercase the text\n",
    "        item = stemmer.stem(item)  # Stem the text\n",
    "        if len(item) < 3:  # If the length of item is lower than 3, remove item\n",
    "            item = ''\n",
    "        list_to_clean[index] = item  # Put item back to the list\n",
    "    cleaned_list = [elem for elem in list_to_clean if elem not in items_to_clean]\n",
    "    # Remove empty items from the list\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "    \n",
    "def get_cleaned_html_documents(folder_file_list):\n",
    "    \"\"\"\n",
    "    Reads the files in folders\n",
    "    Returns a list of list containing cleaned words/data in the html files\n",
    "\n",
    "    Param_1: Output of get_file_names, list of folders and file names\n",
    "    Output_1: List of lists containing string, which are words in the html\n",
    "    \"\"\"\n",
    "    html_parser = MyHTMLParser()\n",
    "    cleaned_documents = []\n",
    "    y = []\n",
    "    with open(folder_file_list, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            x = row['abstract']\n",
    "            \n",
    "            \n",
    "#     plain_html_file = html_parser.html_plain_document  # Get the list of words\n",
    "#     cleaned_html_file = clean_list(plain_html_file) \n",
    "#     #print(plain_html_file)# Clean the list\n",
    "#     cleaned_documents.append(cleaned_html_file)\n",
    "#     html_parser.html_plain_document = []  # Empty the html document\n",
    "    return cleaned_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f2f87a2f3105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_class\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcsv_parser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCsv_Parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCsv_Parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MMP-13 inhibitors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "from app.search_class import Search\n",
    "from csv_parser import Csv_Parser\n",
    "bio = Search()\n",
    "csv = Csv_Parser()\n",
    "title, abstract = bio.query(\"MMP-13 inhibitors\", '10')\n",
    "\n",
    "sentence = csv.find_keyword(abstract)\n",
    "for i in sentence:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initial read\n",
    "folder_file_list = get_file_names(HTML_DATA_FOLDER)\n",
    "cleaned_content = get_cleaned_html_documents(folder_file_list)\n",
    "write_list_to_file('cleaned_content.txt', cleaned_content)\n",
    "frequent_words_removed_content_as_list = remove_frequent_items(cleaned_content_as_list, 75)\n",
    "write_list_to_file('freq_words_removed_content.txt', frequent_words_removed_content)\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from cleaned file, not htmls\n",
    "(cleaned_content_as_list, cleaned_content_as_str) = \\\n",
    "    read_from_cleaned_file('cleaned_content.txt')\n",
    "(frequent_words_removed_content_as_list, frequent_words_removed_content_as_str) = \\\n",
    "    read_from_cleaned_file('freq_words_removed_content.txt')\n",
    "(book_names, authors) = read_authors_book_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(similarity_matrix, tfidf_matrix) = get_similarity_matrix(frequent_words_removed_content_as_str)\n",
    "\n",
    "km_clusters = get_cluster_kmeans(tfidf_matrix, 5)  # KMeans\n",
    "x_pos, y_pos = pca_reduction(similarity_matrix, 10)\n",
    "scatter_clusters(x_pos, y_pos, km_clusters, authors) # Scatter K-means with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dbscan_clusters = get_dbscan_cluster(tfidf_matrix, 1.2)\n",
    "dbscan_clusters = dbscan_clusters + 1  # DBScan clusters start from -1\n",
    "x_pos, y_pos = multidim_scaling(similarity_matrix, 2)  # MultidimScaling\n",
    "scatter_clusters(x_pos, y_pos, dbscan_clusters, authors) # Scatter K-means with PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dendogram(similarity_matrix, book_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = lda_topic_modeling(frequent_words_removed_content_as_list, 5)\n",
    "print(lda_model.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "sns.set(font_scale=1)\n",
    "mask = np.zeros_like(similarity_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = False\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "b = similarity_matrix\n",
    "c = sns.heatmap(b, mask=mask, cmap=\"bwr\", vmax=.8,\n",
    "            square=True, linewidths=0.01,  ax=ax)\n",
    "c.set(xlabel='Document ID', ylabel='Document ID')\n",
    "plt.show()\n",
    "fig = c.get_figure()\n",
    "fig.suptitle('TF-IDF Document Similarity Matrix', fontsize=25)\n",
    "\n",
    "fig.savefig(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
