{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rguigoures.github.io/word2vec_pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/openpose/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "import itertools\n",
    "corpus = []\n",
    "\n",
    "for cat in ['news']:\n",
    "    for text_id in brown.fileids(cat):\n",
    "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id)))\n",
    "        text = ' '.join(raw_text)\n",
    "        text = text.lower()\n",
    "        text.replace('\\n', ' ')\n",
    "        text = re.sub('[^a-z ]+', '', text)\n",
    "        corpus.append([w for w in text.split() if w != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsample frequent words\n",
    "\n",
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "def subsample_frequent_words(corpus):\n",
    "    filtered_corpus = []\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    sum_word_counts = sum(list(word_counts.values()))\n",
    "    word_counts = {word: word_counts[word]/float(sum_word_counts) for word in word_counts}\n",
    "    for text in corpus:\n",
    "        filtered_corpus.append([])\n",
    "        for word in text:\n",
    "            if random.random() < (1+math.sqrt(word_counts[word] * 1e3)) * 1e-3 / float(word_counts[word]):\n",
    "                filtered_corpus[-1].append(word)\n",
    "    return filtered_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = subsample_frequent_words(corpus)\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 475002 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "#Bag of words\n",
    "import numpy as np\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j]))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn  as  nn\n",
    "# import torch.autograd as autograd\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Word2Vec(nn.Module):\n",
    "\n",
    "#     def __init__(self, embedding_size, vocab_size):\n",
    "#         super(Word2Vec, self).__init__()\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "#         self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "#     def forward(self, context_word):\n",
    "#         emb = self.embeddings(context_word)\n",
    "#         hidden = self.linear(emb)\n",
    "#         out = F.log_softmax(hidden)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop loss if it is not improoving\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_percent_gain = min_percent_gain / 100.\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(round(100*gain,2)))\n",
    "        if gain < self.min_percent_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning\n",
    "# vocabulary_size = len(vocabulary)\n",
    "\n",
    "# net = Word2Vec(embedding_size=2, vocab_size=vocabulary_size)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters())\n",
    "# early_stopping = EarlyStopping()\n",
    "# context_tensor_list = []\n",
    "\n",
    "# for target, context in context_tuple_list:\n",
    "#     target_tensor = autograd.Variable(torch.LongTensor([word_to_index[target]]))\n",
    "#     context_tensor = autograd.Variable(torch.LongTensor([word_to_index[context]]))\n",
    "#     context_tensor_list.append((target_tensor, context_tensor))\n",
    "    \n",
    "# while True:\n",
    "#     losses = []\n",
    "#     for target_tensor, context_tensor in context_tensor_list:\n",
    "#         net.zero_grad()\n",
    "#         log_probs = net(context_tensor)\n",
    "#         loss = loss_function(log_probs, target_tensor)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         losses.append(loss.data)\n",
    "#     print(\"Loss: \", np.mean(losses))\n",
    "#     early_stopping.update_loss(np.mean(losses))\n",
    "#     if early_stopping.stop_training():\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
    "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#artificial negative examples\n",
    "from numpy.random import multinomial\n",
    "\n",
    "def sample_negative(sample_size):\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 475002 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "# make this threaded\n",
    "import numpy as np\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4\n",
    "negative_samples = sample_negative(8)\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.embeddings_target(target_word)\n",
    "        emb_context = self.embeddings_context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context)\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out = torch.sum(F.logsigmoid(emb_product))\n",
    "        emb_negative = self.embeddings_context(negative_example)\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out += torch.sum(F.logsigmoid(-emb_product))\n",
    "        return -out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  42203.43\n",
      "Loss:  37249.58\n",
      "Loss gain: 11.74%\n",
      "Loss:  32722.445\n",
      "Loss gain: 22.46%\n",
      "Loss:  28558.176\n",
      "Loss gain: 32.33%\n",
      "Loss:  24816.186\n",
      "Loss gain: 41.2%\n",
      "Loss:  21544.676\n",
      "Loss gain: 42.16%\n",
      "Loss:  18747.59\n",
      "Loss gain: 42.71%\n",
      "Loss:  16378.151\n",
      "Loss gain: 42.65%\n",
      "Loss:  14366.645\n",
      "Loss gain: 42.11%\n",
      "Loss:  12654.258\n",
      "Loss gain: 41.27%\n",
      "Loss:  11187.031\n",
      "Loss gain: 40.33%\n",
      "Loss:  9922.428\n",
      "Loss gain: 39.42%\n",
      "Loss:  8828.775\n",
      "Loss gain: 38.55%\n",
      "Loss:  7876.5806\n",
      "Loss gain: 37.76%\n",
      "Loss:  7043.5796\n",
      "Loss gain: 37.04%\n",
      "Loss:  6313.2974\n",
      "Loss gain: 36.37%\n",
      "Loss:  5669.857\n",
      "Loss gain: 35.78%\n",
      "Loss:  5099.7017\n",
      "Loss gain: 35.25%\n",
      "Loss:  4593.1377\n",
      "Loss gain: 34.79%\n",
      "Loss:  4141.332\n",
      "Loss gain: 34.4%\n",
      "Loss:  3736.6316\n",
      "Loss gain: 34.1%\n",
      "Loss:  3373.6433\n",
      "Loss gain: 33.85%\n",
      "Loss:  3046.3271\n",
      "Loss gain: 33.68%\n",
      "Loss:  2751.6074\n",
      "Loss gain: 33.56%\n",
      "Loss:  2484.887\n",
      "Loss gain: 33.5%\n",
      "Loss:  2243.4785\n",
      "Loss gain: 33.5%\n",
      "Loss:  2023.9703\n",
      "Loss gain: 33.56%\n",
      "Loss:  1824.7343\n",
      "Loss gain: 33.68%\n",
      "Loss:  1643.8716\n",
      "Loss gain: 33.85%\n",
      "Loss:  1479.9067\n",
      "Loss gain: 34.04%\n",
      "Loss:  1331.2716\n",
      "Loss gain: 34.22%\n",
      "Loss:  1196.8634\n",
      "Loss gain: 34.41%\n",
      "Loss:  1075.0974\n",
      "Loss gain: 34.6%\n",
      "Loss:  964.9554\n",
      "Loss gain: 34.8%\n",
      "Loss:  865.79504\n",
      "Loss gain: 34.96%\n",
      "Loss:  776.48846\n",
      "Loss gain: 35.12%\n",
      "Loss:  696.38245\n",
      "Loss gain: 35.23%\n",
      "Loss:  624.5298\n",
      "Loss gain: 35.28%\n",
      "Loss:  560.3078\n",
      "Loss gain: 35.28%\n",
      "Loss:  502.8347\n",
      "Loss gain: 35.24%\n",
      "Loss:  451.53592\n",
      "Loss gain: 35.16%\n",
      "Loss:  405.6188\n",
      "Loss gain: 35.05%\n",
      "Loss:  364.87152\n",
      "Loss gain: 34.88%\n",
      "Loss:  328.42584\n",
      "Loss gain: 34.69%\n",
      "Loss:  295.9131\n",
      "Loss gain: 34.47%\n",
      "Loss:  267.11835\n",
      "Loss gain: 34.15%\n",
      "Loss:  241.2749\n",
      "Loss gain: 33.87%\n",
      "Loss:  218.29016\n",
      "Loss gain: 33.53%\n",
      "Loss:  197.8061\n",
      "Loss gain: 33.15%\n",
      "Loss:  179.39806\n",
      "Loss gain: 32.84%\n",
      "Loss:  162.82106\n",
      "Loss gain: 32.52%\n",
      "Loss:  148.04646\n",
      "Loss gain: 32.18%\n",
      "Loss:  134.6397\n",
      "Loss gain: 31.93%\n",
      "Loss:  122.58794\n",
      "Loss gain: 31.67%\n",
      "Loss:  111.685646\n",
      "Loss gain: 31.41%\n",
      "Loss:  101.87168\n",
      "Loss gain: 31.19%\n",
      "Loss:  92.949005\n",
      "Loss gain: 30.96%\n",
      "Loss:  84.96745\n",
      "Loss gain: 30.69%\n",
      "Loss:  77.67049\n",
      "Loss gain: 30.46%\n",
      "Loss:  70.95293\n",
      "Loss gain: 30.35%\n",
      "Loss:  64.948044\n",
      "Loss gain: 30.13%\n",
      "Loss:  59.60431\n",
      "Loss gain: 29.85%\n",
      "Loss:  54.56324\n",
      "Loss gain: 29.75%\n",
      "Loss:  50.190742\n",
      "Loss gain: 29.26%\n",
      "Loss:  46.138546\n",
      "Loss gain: 28.96%\n",
      "Loss:  42.511337\n",
      "Loss gain: 28.68%\n",
      "Loss:  39.12203\n",
      "Loss gain: 28.3%\n",
      "Loss:  36.267982\n",
      "Loss gain: 27.74%\n",
      "Loss:  33.545006\n",
      "Loss gain: 27.3%\n",
      "Loss:  31.08394\n",
      "Loss gain: 26.88%\n",
      "Loss:  28.956429\n",
      "Loss gain: 25.98%\n",
      "Loss:  26.86461\n",
      "Loss gain: 25.93%\n",
      "Loss:  25.303484\n",
      "Loss gain: 24.57%\n",
      "Loss:  23.610733\n",
      "Loss gain: 24.04%\n",
      "Loss:  22.17377\n",
      "Loss gain: 23.42%\n",
      "Loss:  20.821543\n",
      "Loss gain: 22.49%\n",
      "Loss:  19.591103\n",
      "Loss gain: 22.58%\n",
      "Loss:  18.573795\n",
      "Loss gain: 21.33%\n",
      "Loss:  17.62137\n",
      "Loss gain: 20.53%\n",
      "Loss:  16.669552\n",
      "Loss gain: 19.94%\n",
      "Loss:  15.994607\n",
      "Loss gain: 18.36%\n",
      "Loss:  15.224517\n",
      "Loss gain: 18.03%\n",
      "Loss:  14.433576\n",
      "Loss gain: 18.09%\n",
      "Loss:  13.935953\n",
      "Loss gain: 16.4%\n",
      "Loss:  13.340398\n",
      "Loss gain: 16.59%\n",
      "Loss:  12.814205\n",
      "Loss gain: 15.83%\n",
      "Loss:  12.47539\n",
      "Loss gain: 13.57%\n",
      "Loss:  11.944501\n",
      "Loss gain: 14.29%\n",
      "Loss:  11.516113\n",
      "Loss gain: 13.67%\n",
      "Loss:  11.326738\n",
      "Loss gain: 11.61%\n",
      "Loss:  10.986989\n",
      "Loss gain: 11.93%\n",
      "Loss:  10.607568\n",
      "Loss gain: 11.19%\n",
      "Loss:  10.433837\n",
      "Loss gain: 9.4%\n",
      "Loss:  10.162359\n",
      "Loss gain: 10.28%\n",
      "Loss:  9.994488\n",
      "Loss gain: 9.03%\n",
      "Loss:  9.696063\n",
      "Loss gain: 8.59%\n",
      "Loss:  9.609794\n",
      "Loss gain: 7.9%\n",
      "Loss:  9.423251\n",
      "Loss gain: 7.27%\n",
      "Loss:  9.323501\n",
      "Loss gain: 6.71%\n",
      "Loss:  9.035357\n",
      "Loss gain: 6.81%\n",
      "Loss:  9.090933\n",
      "Loss gain: 5.98%\n",
      "Loss:  8.922245\n",
      "Loss gain: 5.32%\n",
      "Loss:  8.744766\n",
      "Loss gain: 6.21%\n",
      "Loss:  8.651878\n",
      "Loss gain: 4.83%\n",
      "Loss:  8.636762\n",
      "Loss gain: 5.0%\n",
      "Loss:  8.513023\n",
      "Loss gain: 4.59%\n",
      "Loss:  8.436762\n",
      "Loss gain: 3.52%\n",
      "Loss:  8.329462\n",
      "Loss gain: 3.73%\n",
      "Loss:  8.309092\n",
      "Loss gain: 3.79%\n",
      "Loss:  8.201056\n",
      "Loss gain: 3.66%\n",
      "Loss:  8.19417\n",
      "Loss gain: 2.88%\n",
      "Loss:  8.0956955\n",
      "Loss gain: 2.81%\n",
      "Loss:  8.135716\n",
      "Loss gain: 2.57%\n",
      "Loss:  8.051391\n",
      "Loss gain: 1.82%\n",
      "Loss:  8.032519\n",
      "Loss gain: 1.97%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings_target\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
